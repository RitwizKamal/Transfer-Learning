{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infected samples: 13779\n",
      "Uninfected samples: 13779\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "base_dir = os.path.join('F:/0Sem 7/B.TECH PROJECT/0Image data/cell_images')\n",
    "infected_dir = os.path.join(base_dir,'Parasitized')\n",
    "healthy_dir = os.path.join(base_dir,'Uninfected')\n",
    "infected_files = glob.glob(infected_dir+'/*.png')\n",
    "healthy_files = glob.glob(healthy_dir+'/*.png')\n",
    "print(\"Infected samples:\",len(infected_files))\n",
    "print(\"Uninfected samples:\",len(healthy_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f01b3776637f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\newtfgpu\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_print_versions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m from pandas.io.api import (\n\u001b[0m\u001b[0;32m    148\u001b[0m     \u001b[1;31m# excel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[0mExcelFile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\newtfgpu\\lib\\site-packages\\pandas\\io\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgbq\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_gbq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpackers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_msgpack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_msgpack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_parquet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\newtfgpu\\lib\\site-packages\\pandas\\io\\json\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_json\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdumps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_json\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_normalize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson_normalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_table_schema\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbuild_table_schema\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m __all__ = [\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\newtfgpu\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparsers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_normalize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvert_to_line_delimits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_table_schema\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbuild_table_schema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_table_schema\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\newtfgpu\\lib\\site-packages\\pandas\\io\\json\\_normalize.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriters\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvert_json_to_lines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\newtfgpu\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "files_df = pd.DataFrame({\n",
    "    'filename': infected_files + healthy_files,\n",
    "    'label': ['malaria'] * len(infected_files) + ['healthy'] * len(healthy_files)\n",
    "}).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "files_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "train_files, test_files, train_labels, test_labels = train_test_split(files_df['filename'].values,\n",
    "                                                                      files_df['label'].values, \n",
    "                                                                      test_size=0.3, random_state=42)\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(train_files,\n",
    "                                                                    train_labels, \n",
    "                                                                    test_size=0.1, random_state=42)\n",
    "\n",
    "print(train_files.shape, val_files.shape, test_files.shape)\n",
    "print('Train:', Counter(train_labels), '\\nVal:', Counter(val_labels), '\\nTest:', Counter(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from concurrent import futures\n",
    "import threading\n",
    "\n",
    "def get_img_shape_parallel(idx, img, total_imgs):\n",
    "    if idx % 5000 == 0 or idx == (total_imgs - 1):\n",
    "        print('{}: working on img num: {}'.format(threading.current_thread().name,\n",
    "                                                  idx))\n",
    "    return cv2.imread(img).shape\n",
    "  \n",
    "ex = futures.ThreadPoolExecutor(max_workers=None)\n",
    "data_inp = [(idx, img, len(train_files)) for idx, img in enumerate(train_files)]\n",
    "print('Starting Img shape computation:')\n",
    "train_img_dims_map = ex.map(get_img_shape_parallel, \n",
    "                            [record[0] for record in data_inp],\n",
    "                            [record[1] for record in data_inp],\n",
    "                            [record[2] for record in data_inp])\n",
    "train_img_dims = list(train_img_dims_map)\n",
    "print('Min Dimensions:', np.min(train_img_dims, axis=0)) \n",
    "print('Avg Dimensions:', np.mean(train_img_dims, axis=0))\n",
    "print('Median Dimensions:', np.median(train_img_dims, axis=0))\n",
    "print('Max Dimensions:', np.max(train_img_dims, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIMS = (32, 32)\n",
    "\n",
    "def get_img_data_parallel(idx, img, total_imgs):\n",
    "    if idx % 5000 == 0 or idx == (total_imgs - 1):\n",
    "        print('{}: working on img num: {}'.format(threading.current_thread().name,\n",
    "                                                  idx))\n",
    "    img = cv2.imread(img)\n",
    "    img = cv2.resize(img, dsize=IMG_DIMS, \n",
    "                     interpolation=cv2.INTER_CUBIC)\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    return img\n",
    "\n",
    "ex = futures.ThreadPoolExecutor(max_workers=None)\n",
    "train_data_inp = [(idx, img, len(train_files)) for idx, img in enumerate(train_files)]\n",
    "val_data_inp = [(idx, img, len(val_files)) for idx, img in enumerate(val_files)]\n",
    "test_data_inp = [(idx, img, len(test_files)) for idx, img in enumerate(test_files)]\n",
    "\n",
    "print('Loading Train Images:')\n",
    "train_data_map = ex.map(get_img_data_parallel, \n",
    "                        [record[0] for record in train_data_inp],\n",
    "                        [record[1] for record in train_data_inp],\n",
    "                        [record[2] for record in train_data_inp])\n",
    "train_data = np.array(list(train_data_map))\n",
    "\n",
    "print('\\nLoading Validation Images:')\n",
    "val_data_map = ex.map(get_img_data_parallel, \n",
    "                        [record[0] for record in val_data_inp],\n",
    "                        [record[1] for record in val_data_inp],\n",
    "                        [record[2] for record in val_data_inp])\n",
    "val_data = np.array(list(val_data_map))\n",
    "\n",
    "print('\\nLoading Test Images:')\n",
    "test_data_map = ex.map(get_img_data_parallel, \n",
    "                        [record[0] for record in test_data_inp],\n",
    "                        [record[1] for record in test_data_inp],\n",
    "                        [record[2] for record in test_data_inp])\n",
    "test_data = np.array(list(test_data_map))\n",
    "\n",
    "train_data.shape, val_data.shape, test_data.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(1 , figsize = (8 , 8))\n",
    "n = 0 \n",
    "for i in range(16):\n",
    "    n += 1 \n",
    "    r = np.random.randint(0 , train_data.shape[0] , 1)\n",
    "    plt.subplot(4 , 4 , n)\n",
    "    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n",
    "    plt.imshow(train_data[r[0]]/255.)\n",
    "    plt.title('{}'.format(train_labels[r[0]]))\n",
    "    plt.xticks([]) , plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 2\n",
    "EPOCHS = 25\n",
    "INPUT_SHAPE = (32, 32, 3)\n",
    "\n",
    "train_imgs_scaled = train_data / 255.\n",
    "val_imgs_scaled = val_data / 255.\n",
    "\n",
    "# encode text category labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels_enc = le.transform(train_labels)\n",
    "val_labels_enc = le.transform(val_labels)\n",
    "\n",
    "print(train_labels[:6], train_labels_enc[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
    "                                                                zoom_range=0.05, \n",
    "                                                                rotation_range=25,\n",
    "                                                                width_shift_range=0.05, \n",
    "                                                                height_shift_range=0.05, \n",
    "                                                                shear_range=0.05, horizontal_flip=True, \n",
    "                                                                fill_mode='nearest')\n",
    "\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# build image augmentation generators\n",
    "train_generator = train_datagen.flow(train_data, train_labels_enc, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_generator = val_datagen.flow(val_data, val_labels_enc, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = 0\n",
    "sample_generator = train_datagen.flow(train_data[img_id:img_id+1], train_labels[img_id:img_id+1],\n",
    "                                      batch_size=1)\n",
    "sample = [next(sample_generator) for i in range(0,5)]\n",
    "fig, ax = plt.subplots(1,5, figsize=(16, 6))\n",
    "print('Labels:', [item[1][0] for item in sample])\n",
    "l = [ax[i].imshow(sample[i][0][0]) for i in range(0,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = tf.keras.applications.densenet.DenseNet(include_top=False, weights='imagenet', \n",
    "                                        input_shape=INPUT_SHAPE)\n",
    "# Freeze the layers\n",
    "vgg.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in vgg.layers:\n",
    "    if layer.name in ['block5_conv1', 'block4_conv1']:\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "    \n",
    "base_vgg = vgg\n",
    "base_out = base_vgg.output\n",
    "pool_out = tf.keras.layers.Flatten()(base_out)\n",
    "hidden1 = tf.keras.layers.Dense(512, activation='relu')(pool_out)\n",
    "drop1 = tf.keras.layers.Dropout(rate=0.3)(hidden1)\n",
    "hidden2 = tf.keras.layers.Dense(512, activation='relu')(drop1)\n",
    "drop2 = tf.keras.layers.Dropout(rate=0.3)(hidden2)\n",
    "\n",
    "out = tf.keras.layers.Dense(1, activation='sigmoid')(drop2)\n",
    "\n",
    "model = tf.keras.Model(inputs=base_vgg.input, outputs=out)\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "adam = Adam(lr=0.0001)\n",
    "\n",
    "model.compile(optimizer=adam,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "print(\"Total Layers:\", len(model.layers))\n",
    "print(\"Total trainable layers:\", sum([1 for l in model.layers if l.trainable]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "val_steps_per_epoch = val_generator.n // val_generator.batch_size\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=train_steps_per_epoch, epochs=EPOCHS,\n",
    "                              validation_data=val_generator, validation_steps=val_steps_per_epoch, \n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "t = f.suptitle('Basic CNN Performance', fontsize=12)\n",
    "f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "max_epoch = len(history.history['accuracy'])+1\n",
    "epoch_list = list(range(1,max_epoch))\n",
    "ax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\n",
    "ax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\n",
    "ax1.set_xticks(np.arange(1, max_epoch, 5))\n",
    "ax1.set_ylabel('Accuracy Value')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_title('Accuracy')\n",
    "l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "ax2.plot(epoch_list, history.history['loss'], label='Train Loss')\n",
    "ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\n",
    "ax2.set_xticks(np.arange(1, max_epoch, 5))\n",
    "ax2.set_ylabel('Loss Value')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_title('Loss')\n",
    "l2 = ax2.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs_scaled = test_data/255.\n",
    "test_labels_enc = le.transform(test_labels)\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(train_imgs_scaled, train_labels_enc, verbose=0)\n",
    "_, test_acc = model.evaluate(test_imgs_scaled, test_labels_enc, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
